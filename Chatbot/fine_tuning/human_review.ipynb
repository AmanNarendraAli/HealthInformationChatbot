{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from med_assist.config import set_project_wd\n",
    "\n",
    "set_project_wd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "df_raw = pd.read_csv(\"resources/med_assist_training_dataset_0301-1858.csv\")\n",
    "\n",
    "inputs = df_raw['input_prompt']\n",
    "outputs = df_raw['output_output']\n",
    "\n",
    "contexts = inputs.str.extract(r\"((?<=Context: \\[\\[).*(?=\\]\\]))\")[0]\n",
    "questions = inputs.str.extract(r\"((?<=Question: ).*(?=\\n))\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disclaimers to be removed or replaced\n",
    "\n",
    "DISCLAIMERS = {\n",
    "    r\"I hope this information (helps|is helpful)\\b.{0,120}$\": \"\",\n",
    "    r\"Please note that .* based .* context\\b.{0,200}$\": \"\",\n",
    "    r\"(Additionally|Therefore|However|It is\\b.*\\b(recommended|important|best|essential)|As with any|While|If you\\b.*\\bconcerns|Remember)\\b.*\\bconsult\\b.*\\b(healthcare|medical) professional\\b.{0,200}$\": \"\",\n",
    "    r\"(?<=\\b[Bb]ased )on the information provided( in the [a-z]+\\b)?,?\": \"on the study,\",\n",
    "    r\"(?<=\\b[Bb]ased )on the( provided)? context( provided)?,?\": \"on the study,\",\n",
    "    r\"\\n.*(harmful|unethical|illegal)\\b.*\\bcontent\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*[Bb]ased\\b.*context\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*constitute medical advice\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*no prior knowledge\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*no additional information\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*(socially unbiased|positive in nature)\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*\\brelevant\\b.*\\bquestion\\b.{0,200}\": \"\",\n",
    "    r\"\\n.*\\bincludes.*\\bdetails\\b.*\\bcontext\\b.{0,200}\": \"\",\n",
    "    r\"\\bcontext\\b\": \"study\",\n",
    "    r\"\\bno information available\\b.{0,3}$\": \"\",\n",
    "    r\"\\bNote\\b.{0,3}$\": \"\",\n",
    "    r\"\\b[Ss]ource: [Cc]ontext\\b\": \"\"\n",
    "}\n",
    "\n",
    "i=-1\n",
    "\n",
    "distinct_disclaimers = outputs[outputs.str.contains(pat=list(DISCLAIMERS.keys())[i], regex=True)].str.extract(f\"({list(DISCLAIMERS.keys())[i]})\").drop_duplicates()\n",
    "\n",
    "for i in range(len(distinct_disclaimers[0])):\n",
    "    pprint(distinct_disclaimers[0].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disclaimer, fillin in DISCLAIMERS.items():\n",
    "    outputs = outputs.str.replace(disclaimer, fillin, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output ids to be replaced (as a whole) with a \"no information available\" answer (list prepared after checking suspicious phrases)\n",
    "\n",
    "NO_INFO_ANSWER = \"No information available on this topic. Please try to rephrase the question.\"\n",
    "\n",
    "with open(\"resources/no_info_outputs.yaml\", \"r\") as file:\n",
    "    no_info_outputs = yaml.safe_load(file).get('no_info_outputs')\n",
    "\n",
    "outputs.loc[no_info_outputs] = NO_INFO_ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suspiocious answers to check\n",
    "\n",
    "TO_CHECK_PHRASES = [\n",
    "    r\"^.{0,100}\\bno\\b.{0,20}\\binformation\\b.{0,20}\\bavailable\\b\",\n",
    "    r\"\\bno\\b.*\\binformation\\b.*\\bavailable\\b\",\n",
    "    r\"source: context\"\n",
    "    ]\n",
    "\n",
    "i=0\n",
    "\n",
    "to_check_idx = outputs. \\\n",
    "    str.replace(r\"\\n\", \" \", regex=True). \\\n",
    "    str.contains(TO_CHECK_PHRASES[i], regex=True, case=False). \\\n",
    "    loc[lambda s: s == True]. \\\n",
    "    loc[lambda s: ~s.index.isin(no_info_outputs)]. \\\n",
    "    index\n",
    "\n",
    "for idx in to_check_idx:\n",
    "    pprint(idx)\n",
    "    pprint(questions[idx])\n",
    "    pprint(outputs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise list indicators\n",
    "\n",
    "has_bullet = \\\n",
    "    (outputs.str.count(r\"[0-9]\\. \") < 3) & \\\n",
    "    (outputs.str.count(r\"[\\*] \") >= 1)\n",
    "\n",
    "has_bullet_updated = has_bullet\n",
    "n_has_bullet = sum(has_bullet)\n",
    "num_replace = 0\n",
    "\n",
    "while n_has_bullet:\n",
    "    num_replace += 1 \n",
    "    str_replace = f\"{num_replace}. \"\n",
    "\n",
    "    outputs.loc[has_bullet_updated] = \\\n",
    "        outputs.loc[has_bullet_updated]. \\\n",
    "            str.replace(r\"\\* \", str_replace, n=1, regex=True)\n",
    "\n",
    "    has_bullet_updated = has_bullet & (outputs.str.count(r\"[\\*] \") >= 1)\n",
    "    n_has_bullet = sum(has_bullet_updated)\n",
    "\n",
    "print(f\"Done in {num_replace} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "\n",
    "outputs = outputs.str.replace(r\"[\\n ]*$\", \"\", regex=True)\n",
    "outputs = outputs.str.replace(r\"[^A-Za-z]+$\", \".\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting prompts\n",
    "\n",
    "inputs = inputs.str.replace(\n",
    "    pat  = \"Always return a concise list of facts regarding the question based on the provided context\",\n",
    "    repl = \"Always return a concise numbered list of facts regarding the question based on the provided context\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browsing outputs\n",
    "\n",
    "i_iter = iter(range(len(outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(i_iter)\n",
    "\n",
    "pprint(i)\n",
    "pprint(questions[i])\n",
    "pprint(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags for stratified sampling\n",
    "\n",
    "has_no_info = outputs.index. \\\n",
    "    to_series(). \\\n",
    "    apply(lambda idx: idx in no_info_outputs). \\\n",
    "    rename(\"has_no_info\")\n",
    "\n",
    "is_changed = (outputs != df_raw['output_output']).rename(\"is_changed\")\n",
    "\n",
    "stratas = pd.concat([has_no_info, is_changed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and test dataset\n",
    "\n",
    "train_size = 2000\n",
    "\n",
    "df_output_sft = pd.DataFrame(\n",
    "    zip(inputs, outputs), \n",
    "    columns=[\"prompt\", \"output\"])\n",
    "\n",
    "df_train_sft, df_test_sft = train_test_split(\n",
    "    df_output_sft, \n",
    "    train_size=train_size, \n",
    "    test_size=len(questions)-train_size, \n",
    "    random_state=420, \n",
    "    shuffle=True, \n",
    "    stratify=stratas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write human reviewed outputs to file for SFT\n",
    "\n",
    "df_train_sft.to_csv(\"resources/training_dataset_sft.csv\", index=False)\n",
    "df_test_sft.to_csv(\"resources/validation_dataset_sft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for DPO\n",
    "\n",
    "df_output_dpo = pd.DataFrame(\n",
    "    zip(inputs, outputs, df_raw['output_output']), \n",
    "    columns=[\"prompt\", \"chosen\", \"rejected\"]\n",
    "    )[is_changed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_dpo.to_csv(\"resources/training_dataset_dpo.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
